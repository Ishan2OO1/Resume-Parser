{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08571706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [WinError 10060] A connection attempt failed because\n",
      "[nltk_data]     the connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n",
      "[nltk_data] Error loading maxent_ne_chunker: <urlopen error [WinError\n",
      "[nltk_data]     10060] A connection attempt failed because the\n",
      "[nltk_data]     connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n",
      "[nltk_data] Error loading words: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ryan Nelson Arjuvo Limited\n",
      "Skills: {'Python', 'C', 'Machine learning', 'machine learning', 'Machine Learning'}\n",
      "{'Stanford University'}\n"
     ]
    }
   ],
   "source": [
    "# Ishan Chaudhary (20103059) -- B3\n",
    "\n",
    "\n",
    "\n",
    "import nltk  #natural language toolkit\n",
    "import re\n",
    "import subprocess # spawn new process\n",
    "from pdfminer.high_level import extract_text\n",
    "import requests\n",
    "\n",
    "#re: a way to work with patterns and text.\n",
    "#pdfminer.high_level: A library for extracting text from PDF files.\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#NLTK data packages\n",
    "# pre-trained models for tokenization.\n",
    "# part-of-speech tagging - labeling each word\n",
    "# NER -process of identifying and categorizing named entities \n",
    "# \"a\", \"an\", \"the\", \"and\", \"or\"\n",
    "\n",
    "\n",
    "\n",
    "# Extracting text from the pdf\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "# Extracting the names from the text\n",
    "\n",
    "def extract_names(txt):\n",
    "    person_names = []\n",
    "\n",
    "    for sent in nltk.sent_tokenize(txt): #to split the document into sentences.NE recognised at sentence level\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))): #sentence into individual words, then applies part-of-speech tagging, then ne of chunk\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':  # chunk represents a contiguous sequence of words that have been labeled with the same named entity tag\n",
    "                person_names.append(\n",
    "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves()) # leaves of the chunk (i.e., the individual words)\n",
    "                )\n",
    "    return person_names\n",
    "\n",
    "#Extracting the phone number\n",
    "\n",
    "PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "# may start with + or (\n",
    "#start 1-9\n",
    "# 8 characters\n",
    "# end with 0-9\n",
    "\n",
    "\n",
    "\n",
    "def extract_phone_number(resume_text):\n",
    "    phone = re.findall(PHONE_REG, resume_text) #search the input text for all substrings that match the PHONE_REG pattern.\n",
    "\n",
    "    if phone: #if phone no. was found\n",
    "        number = ''.join(phone[0])#extracts the first number from the phone list, joins its individual components into a single string\n",
    "\n",
    "        if resume_text.find(number) >= 0 and len(number) < 15:\n",
    "            return number\n",
    "    return None\n",
    "\n",
    "#Extracting email address\n",
    "\n",
    "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+') #@ domain name , then . Top Level domain\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "\n",
    "def extract_emails(resume_text):\n",
    "    return re.findall(EMAIL_REG, resume_text)\n",
    "\n",
    "\n",
    "#Extracting skills\n",
    "\n",
    "SKILLS_DB = [\n",
    "    'machine learning',\n",
    "    'data science',\n",
    "    'python',\n",
    "    'word',\n",
    "    'excel',\n",
    "    'English',\n",
    "    'c',\n",
    "    'sql',\n",
    "    'matlab',\n",
    "    'javascript',\n",
    "    'office',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_skills(input_text):\n",
    "    # stop words list\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))  #stopwords from english txt file\n",
    "\n",
    "    # tokenizing involves splitting sentences and words from the body of the text.\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    "\n",
    "\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words] #remove the word token if it is in stop words\n",
    "\n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "\n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    #below line\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "    # nltk.everygrams(sequence, min_len, max_len)\n",
    "    # we create a set to keep the results in.\n",
    "    found_skills = set()\n",
    "\n",
    "    # we search for each token in our skills database\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in SKILLS_DB: #lowercase\n",
    "            found_skills.add(token)\n",
    "\n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in SKILLS_DB:\n",
    "            found_skills.add(ngram)\n",
    "## Adding both tokens and ngrams to found skill set.\n",
    "    return found_skills\n",
    "\n",
    "\n",
    "#Extracting Education\n",
    "\n",
    "RESERVED_WORDS = [\n",
    "    'school',\n",
    "    'college',\n",
    "    'univers',\n",
    "    'academy',\n",
    "    'faculty',\n",
    "    'institute',\n",
    "    'faculdades',\n",
    "    'Schola',\n",
    "    'schule',\n",
    "    'lise',\n",
    "    'lyceum',\n",
    "    'lycee',\n",
    "    'polytechnic',\n",
    "    'kolej',\n",
    "    'Ã¼nivers',\n",
    "    'okul',\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_education(input_text):\n",
    "    organizations = []\n",
    "\n",
    "    # first get all the organization names using nltk\n",
    "    for sent in nltk.sent_tokenize(input_text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
    "                organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
    "\n",
    "    # we search for each bigram and trigram for reserved words\n",
    "    # (college, university etc...)\n",
    "    education = set()\n",
    "    for org in organizations:\n",
    "        for word in RESERVED_WORDS:\n",
    "            if org.lower().find(word) >= 0: #returns a non-negative integer, which means that the reserved word was found in the organization string, \n",
    "                education.add(org)\n",
    "\n",
    "    return education\n",
    "\n",
    "\n",
    "if __name__ == '__main__': #to execute some code only if the file was run directly, and not imported.\n",
    "    text = extract_text_from_pdf('resume.pdf')\n",
    "    education_information = extract_education(text)\n",
    "    skills = extract_skills(text)\n",
    "    names = extract_names(text)\n",
    "    phone_number = extract_phone_number(text)\n",
    "    emails = extract_emails(text)\n",
    "\n",
    "\n",
    "if emails:\n",
    "        print(emails[0])\n",
    "\n",
    "if names:\n",
    "        print(names[0] + ' ' + names[1]) # first and second name\n",
    "\n",
    "if skills:\n",
    "    print('Skills: ' + str(skills))\n",
    "\n",
    "if education_information:\n",
    "    print(education_information)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b40431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer in d:\\anaconda\\lib\\site-packages (20191125)\n",
      "Requirement already satisfied: pycryptodome in d:\\anaconda\\lib\\site-packages (from pdfminer) (3.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pdfminer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f6c5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer in d:\\anaconda\\lib\\site-packages (20191125)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pycryptodome in d:\\anaconda\\lib\\site-packages (from pdfminer) (3.17)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pdfminer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81bac23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Using cached pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "Collecting cryptography>=36.0.0\n",
      "  Using cached cryptography-40.0.2-cp36-abi3-win_amd64.whl (2.6 MB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in d:\\anaconda\\lib\\site-packages (from pdfminer.six) (2.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in d:\\anaconda\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.0)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pycparser in d:\\anaconda\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
      "Installing collected packages: cryptography, pdfminer.six\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 3.4.8\n",
      "    Uninstalling cryptography-3.4.8:\n",
      "      Successfully uninstalled cryptography-3.4.8\n",
      "Successfully installed cryptography-40.0.2 pdfminer.six-20221105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b9379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
